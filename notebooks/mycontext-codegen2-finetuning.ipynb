{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MyContext AI - CodeGen2-1B Fine-tuning\n",
        "\n",
        "Fine-tune CodeGen2-1B on Google Colab's free T4 GPU using training data from the Intent Dictionary.\n",
        "\n",
        "**Total Cost: $0** ðŸŽ‰\n",
        "\n",
        "## What This Does\n",
        "- Trains CodeGen2-1B to generate React/TypeScript components from natural language\n",
        "- Uses LoRA (Low-Rank Adaptation) for memory efficiency\n",
        "- Deploys to Hugging Face Hub for free inference\n",
        "- Validates the MyContext AI concept with code-native model\n",
        "\n",
        "## Key Advantages\n",
        "- **Code-native**: Trained on Python, JavaScript, TypeScript\n",
        "- **2048 context**: vs GPT-2's 1024 tokens\n",
        "- **No repetition**: Code models don't loop like text models\n",
        "- **Memory efficient**: 1B parameters fits easily in Colab free tier\n",
        "- **Same cost**: $0 on Google Colab\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 1: Environment Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "# Install required packages\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes huggingface_hub tensorboard\n",
        "\n",
        "print(\"âœ… Packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 2: Hugging Face Login\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Login to Hugging Face (you'll need to get a token from https://huggingface.co/settings/tokens)\n",
        "notebook_login()\n",
        "\n",
        "print(\"âœ… Logged in to Hugging Face!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 3: Load StarCoder2-3B Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear cache\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Use StarCoder2-3B for code generation\n",
        "model_name = \"bigcode/starcoder2-3b\"\n",
        "print(f\"ðŸ”„ Loading {model_name}...\")\n",
        "\n",
        "# Load in 8-bit for memory efficiency\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"âœ… Model loaded: {model.num_parameters():,} parameters\")\n",
        "print(f\"ðŸ’¾ GPU memory used: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 4: Load Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load your uploaded file (upload starcoder2_training_data.jsonl first)\n",
        "dataset = load_dataset(\"json\", data_files=\"starcoder2_training_data.jsonl\")\n",
        "\n",
        "# Split train/val\n",
        "train_test_split = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_dataset = train_test_split[\"train\"]\n",
        "eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "print(f\"ðŸ“Š Train examples: {len(train_dataset)}\")\n",
        "print(f\"ðŸ“Š Eval examples: {len(eval_dataset)}\")\n",
        "\n",
        "# Show sample\n",
        "print(\"\\nðŸ“‹ Sample training example:\")\n",
        "print(train_dataset[0][\"text\"][:300] + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 5: Tokenize Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=2048,  # StarCoder2 supports 16k, use 2k for training\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "print(\"ðŸ”„ Tokenizing datasets...\")\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "eval_dataset = eval_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "print(\"âœ… Datasets tokenized successfully\")\n",
        "print(f\"Train dataset columns: {train_dataset.column_names}\")\n",
        "print(f\"Eval dataset columns: {eval_dataset.column_names}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 6: Setup LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# LoRA configuration for StarCoder2\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # StarCoder2 attention\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Applying LoRA adapters...\")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(f\"ðŸ’¾ GPU memory after LoRA: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cell 7: Configure Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Training arguments optimized for StarCoder2\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mycontext-starcoder2-lora\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-4,  # Higher for code models\n",
        "    warmup_steps=100,\n",
        "    logging_steps=25,\n",
        "    eval_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    dataloader_num_workers=0,\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=\"faraja/mycontext-starcoder2\",\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "# Data collator for language modeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer configured successfully\")\n",
        "print(f\"Repository: {training_args.hub_model_id}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
